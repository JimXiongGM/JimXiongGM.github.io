{"pages":[],"posts":[{"title":"Hello World","text":"摘要部分. Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/04/29/hello-world/"},{"title":"DIALOGPT  Large-Scale Generative Pre-training for Conversational Response Generation","text":"12 SummaryDatasetsModel StructureLoss functionMetricsTrain detailsQuestionsRethinkingsKnowledge PointsRelated Resources","link":"/2020/08/02/DIALOGPT%20%20Large-Scale%20Generative%20Pre-training%20for%20Conversational%20Response%20Generation/"},{"title":"PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable","text":"12 SummaryDatasetsModel StructureLoss functionMetricsTrain detailsQuestionsRethinkingsKnowledge PointsRelated Resources","link":"/2020/08/02/PLATO%20Pre-trained%20Dialogue%20Generation%20Model%20with%20Discrete%20Latent%20Variable/"},{"title":"PLATO-2 Towards Building an Open-Domain Chatbot via Curriculum Learning","text":"12 SummaryDatasetsModel StructureLoss functionMetricsTrain detailsQuestionsRethinkingsKnowledge PointsRelated Resources","link":"/2020/07/30/PLATO-2%20Towards%20Building%20an%20Open-Domain%20Chatbot%20via%20Curriculum%20Learning/"},{"title":"Recipes for building an open-domain chatbot","text":"2020年4月30日，Facebook 的人工智能和机器学习部门 Facebook AI Research（FAIR）详细介绍了一个名为 Blender 的综合人工智能聊天机器人框架。 FAIR 声称，目前可以在 GitHub 上以开源方式获得的 Blender，是有史以来最强大的开放域聊天机器人，它比生成对话的现有方法更有“人情味”。 FAIR 表示，Blender 是多年研究的巅峰之作，它将移情、知识和个性结合到一个系统中。为此，受益于改进的解码和技能混合技术的基础模型，包含了多达 94 亿个参数（定义特定问题上的技能配置变量），是之前系统的 3.6 倍。 1234Stephen Roller Emily Dinan Naman Goyal Da JuMary Williamson Yinhan Liu∗ Jing Xu Myle OttKurt Shuster Eric M. Smith Y-Lan Boureau Jason WestonFacebook AI Research Summary提出blender. 流程: Pre-training Ranking models –&gt; Pre-training Generative models –&gt; Fine-tuning. 预训练的两个部分使用的是Reddit数据集, Fine-tuning部分使用的是4个数据集. Datasets pushshift.io Reddit. 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens, 一共有1456亿BPE tokens. meena是450亿 word. ConvAI2 (用于improved consistency) Wizard of Wikipedia (WoW) (知识型对话) Empathetic Dialogues (ED) (增加移情) Blended Skill Talk dataset (上述三者的混合) Model Structure三个模型, 90M, 2.7B and 9.4B parameter models. encoder部分: poly-encoder architecture Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. ![image-20200801003146984](Recipes for building an open-domain chatbot/image-20200801003146984.png) decoder部分: Retrieval (纯检索, 无生成) 给定对话历史(上下文)作为输入，检索系统通过对大量候选responses进行评分并输出得分最高的一个。通常，使用所有的训练集responses作为候选集。 Generator (纯生成) Seq2Seq Transformer architecture ,三个模型, 90M, 2.7B and 9.4B parameter models. Retrieve and Refine (检索拼一起, 生成) 生成模型可能有问题, 例如怪话. 这里combine a retrieval step before generation, referred to as a retrieve and refine model (Weston et al., 2018). 这里使用两个变种: dialogue retrieval and knowledge retrieval dialogue retrieval首先使用1中的Retriever, 获取得分最高的历史上的response, 和当前用户的input拼接, 形成新的输入, 再去生成一个response. knowledge retrievalcondition the generation on the retrieved knowledge, 相当于使用检索器获取知识库中的句子,和当前用户的input拼接, 形成新的输入, 再去生成一个response. 具体, 用的是 TF-IDF-based inverted index lookup over a Wikipedia dump. decoder方式 Beam Search Sampling. top-k sampling 和 Meena 的sample-and-rank sampling. Loss functions Ranking for Retrieval. 数据集中的下一句话是明确的. 把下一句当做答案, 能训练一个检索器. Likelihood Training for Generation. 使用标准的Maximum Likelihood Estimation (MLE) 方法训练生成模型. α-blending for Retrieve and Refine. 说的是将检索到的句子以α%的概率替换为答案句子. Unlikelihood training for generation. 对负采样的句子做反似然. MetricsACUTE-Eval. evaluators are asked to make pairwise evaluations of complete dialogues. 两两对比型, 整个对话session判断, 哪一个更好. 见原文fig4 Train detailsOur 256M parameter ranking model is identical to the pre-trained model released by &lt;Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring.&gt; 使用 Fairseq. 使用Megatron-LM style model parallelism. 使用ParlAI toolkit微调. 预处理部分删掉了一些语料. The author is a known bot. It comes from a known non-English sub-reddit. The comment is marked as removed / deleted. It is longer than 2048 characters and does not contain spaces. It is longer than 128 BPE tokens. It is shorter than 5 characters. It contains a URL. It starts with a non-ASCII character. It is further than depth 7 in the thread. QuestionsSelf-Chat ACUTE-Eval 怎么评价? Rethinkings暂无. Knowledge Pointspoly-encoder architecture Related ResourcesFacebook 开源聊天机器人 Blender，经 94 亿个参数强化训练，更具“人情味”","link":"/2020/08/01/Recipes%20for%20building%20an%20open-domain%20chatbot/"},{"title":"DialoGPT Large-Scale Generative Pre-training for Conversational Response Generation","text":"DialoGPT Large-Scale Generative Pre-training for Conversational Response Generation title: test_jimdate: 2020-04-29 10:37:25tags: Testing Another Tagcategories: hexo1 摘要部分. 正文","link":"/2020/07/30/DialoGPT%20Large-Scale%20Generative%20Pre-training%20for%20Conversational%20Response%20Generation/"},{"title":"test_jim","text":"摘要部分. 正文","link":"/2020/04/29/Resume%20-%20%E5%89%AF%E6%9C%AC%20(10)/"},{"title":"Pre-trained Dialogue Generation Model with Discrete Latent Variable","text":"Pre-trained Dialogue Generation Model with Discrete Latent Variable title: test_jimdate: 2020-04-29 10:37:25tags: Testing Another Tagcategories: hexo1 摘要部分. 正文","link":"/2020/07/30/Pre-trained%20Dialogue%20Generation%20Model%20with%20Discrete%20Latent%20Variable/"},{"title":"test_jim","text":"摘要部分. 正文","link":"/2020/04/29/Resume%20-%20%E5%89%AF%E6%9C%AC%20(7)/"},{"title":"test_jim","text":"摘要部分. 正文","link":"/2020/04/29/Resume%20-%20%E5%89%AF%E6%9C%AC%20(8)/"},{"title":"The Evolved Transformer","text":"1David R. So 1 Chen Liang 1 Quoc V. Le 1 Summary本文的目标是: Our goal is to apply NAS to search for a better alternative to the Transformer. DatasetsModel StructureLoss functionMetricsTrain detailsQuestionsRethinkingsKnowledge PointsRelated Resources","link":"/2020/08/01/The%20Evolved%20Transformer/"},{"title":"test_jim","text":"摘要部分. 正文","link":"/2020/04/29/Resume%20-%20%E5%89%AF%E6%9C%AC%20(9)/"},{"title":"Towards a Human-like Open-Domain Chatbot","text":"谷歌的研究者提出了一个新的聊天机器人，名为 Meena。这是一个有着 26 亿参数的端到端神经对话模型，也就是 GPT-2 模型最大版本（15 亿参数）的 1.7 倍。通过实验可以看到，Meena 比现有的 SOTA 聊天机器人能够更好地完成对话，对话内容显得更为具体、清楚。 1234Daniel Adiwardana Minh-Thang Luong David R. So Jamie HallNoah Fiedel Romal Thoppilan Zi Yang Apoorv KulshreshthaGaurav Nemade Yifeng Lu Quoc V. LeGoogle Research, Brain Team SummaryMeena是一个基于Evolved Transformer 的端到端模型. 在341GB的公开社交媒体对话数据上进行预训练. 输入是 (最多7轮的context, response), 损失函数是 cross-entropy loss. Dataset400亿word的公开社交媒体对话数据, BPE之后有 8K BPE subwords. meena做了预处理: 输入是 all turns of the context (up to 7) , 输出是the response, 即有867M个(context, response)对. 数据量: 341GB of text (40B words), 对比下GPT-2 只有 40GB of Internet text (8 million web pages). Model Structure用的是Evolved Transformer (ET) seq2seq model, 包含1 ET encoder block and 13 ET decoder blocks. TheEvolved Transformer is an evolutionary NAS architecture. ![image-20200801000441386](Towards a Human-like Open-Domain Chatbot/image-20200801000441386.png) Loss function在next token上最小化perplexity. Metrics提出了新的人工评价指标用于评价多轮对话 Sensibleness and Specificity Average (SSA). SSA衡量两个能力: making sense and being specific. 前者说的是对话要合理. 人与人之间的对话能达到97%. 文中有个GenericBot, 对所有的问题回答“I don’t know”, 所有的陈述回答回答”OK”, sensible rate能到70%, DialoGPT才62%….后者评价的是whether a response is specific given the context. 例如 “I love tennis,”回答是 “That’s nice,”则不行, 回答是“Me too, I can’t get enough of Roger Federer!”则标1. SSA的评价方式还分为static and interactive. 前者是有个1477 multi-turn conversations 的数据集. 相同的输入, 能够横向比较模型. 如下图. 后者是人类随意和机器聊天. 长度限制: 14 turns (7 from chatbot) and at most 28 turns. ![Table 4: Sample responses from static evaluation](Towards a Human-like Open-Domain Chatbot/image-20200731233642038.png) 人类的SSA能到86%, end-to-end trained Meena能到72%, full version of Meena (with a filtering mechanism and tuned decoding) 能到79%. 此外还有客观指标, 困惑度, Menna能到10.2. Train details30 days on a TPUv3 Pod (2,048 TPU cores). QuestionsSurprised, SSA metric shows strong correlation with Meena’s perplexity, 就是说, the better that Meena fit its training data, the more sensible and specific its chat responses became. 为什么惊讶? recent research found a poor correlation between human evaluation scores and automatic metrics such as BLEU. 为什么人类的评价和客观指标存在不一致的情况呢? Rethinking暂无. Knowledge PointsThe Evolved TransformerLarge-scale evolution of image classifiers Related Resources不再鹦鹉学舌：26亿参数量，谷歌开放领域聊天机器人近似人类水平","link":"/2020/07/31/Towards%20a%20Human-like%20Open-Domain%20Chatbot/"},{"title":"You are how you read","text":"2020年5月14日，沈向洋博士在全球创新学院（GIX）课程上曾做了一场线上公开课《You are how you read》，分享他对于科研论文阅读、撰写的宝贵经验，引起一时轰动。由于围观网友太多，导致会议系统崩溃，众多网友无法接入观看，不免唏嘘不已。近日，为满足广大网友的需求，微软亚洲研究院将沈向洋博士的报告视频公开。 这场报告亮点纷呈，引人深思。 演讲总结 quick skimming. Answer the following questions. You can just divide the paragraphs. Why author？（task）what？（findings） Why you？（need）So what？（conclusion） why now？（context）what now？（Perspective） critical reading. If the authors attempt to solve a problem are they solving the right problem? are there simple solutions not have considered? what are the limitations of the solution? if the authors present data, do they have right data to substantiate argument? do they gather it in the correct manner? do they interpret data in a reasonable manner? are there other more compelling data sets? Are the authors’ assumptions reasonable? Is the logic clear &amp; justifiable, given assumption? is there a flaw in the reasoning? creative reading. what are the good ideas in this paper? Improvements that might make important practical differences? Applications/extensions that the authors might not have thought of? Can they be generalized futher? If you were going to start doing research from this paper, what next thing would you do? Write a one-page review. Your summary: Key idea in a sentence or two. Learning from critical reading: any problems? Learning from creative reading: idea for next year’s conference? Your opinion: Do you recommend this paper？ Share this review with others？Do a presentation？ Analyze with a list of questions客观 What’s main claim? Key idea? What’s key limitation? Is there code available? Data? Is the idea near? Is it counter-intuitive? Is the contribution significant enough? Is the experimentation good? Manual tuning? 主观 Are there important related papres I missed? Is it useful to my work e.g. product dev? Is this a paper worth following? What do other people think about this paper? Who are dominating this area of research? What question should I ask authors? Related Resources知乎: 沈向洋：读论文的三个层次知乎: 前言-How to read papers","link":"/2020/07/31/You%20are%20how%20you%20read/"},{"title":"项目总结","text":"对以往项目的总结. 调研任务型对话调研成果: 一个ppt, 下载链接: https://www.jianguoyun.com/p/De7zGMUQ8ITOCBjW6LAD (访问密码: 0rmb7v) 整理中文NLP工具https://github.com/JimXiongGM/NLP_toolkits_chinese 在实习期间, 对常用的文本分类算法进行了总结, 实现了从数据输入到启动推理服务的全流程. 未来还有更多功能等待添加. HotpotQA实验https://gitee.com/JimXiongGM/HotpotQA_GNN 完整复现了HotpotQA distract setting榜单第一的模型HGN的图神经网络部分, 尝试在其基础上做进一步的改进, 但是没有超过原作. 大数据存储与分析课程设计https://github.com/JimXiongGM/BigDataProject 本项目是课程设计, 我在申请的4台阿里云服务器上搭建了Hadoop以及相关生态环境, 并尝试使用Hadoop streaming + spark在kaggle数据: Amazon book salesrank 上完成了简单的数据统计. 推荐系统课程设计https://github.com/JimXiongGM/MovieRecOnline 本人在项目中充当小组长的角色, 主要工作是搭建好Django, spark, mysql框架, 并设计组员分工和模块之间的输入输出.","link":"/2020/08/02/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"},{"title":"Reading papers","text":"摘要部分. 正文","link":"/2020/04/29/Resume%20-%20%E5%89%AF%E6%9C%AC/"},{"title":"Resume","text":"Resume","link":"/2020/07/30/Resume/"}],"tags":[{"name":"Testing","slug":"Testing","link":"/tags/Testing/"},{"name":"Another Tag","slug":"Another-Tag","link":"/tags/Another-Tag/"},{"name":"Facebook","slug":"Facebook","link":"/tags/Facebook/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"Menna","slug":"Menna","link":"/tags/Menna/"},{"name":"方法论","slug":"方法论","link":"/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"目录","slug":"目录","link":"/tags/%E7%9B%AE%E5%BD%95/"}],"categories":[{"name":"hexo1","slug":"hexo1","link":"/categories/hexo1/"},{"name":"对话","slug":"对话","link":"/categories/%E5%AF%B9%E8%AF%9D/"},{"name":"周边与八卦","slug":"周边与八卦","link":"/categories/%E5%91%A8%E8%BE%B9%E4%B8%8E%E5%85%AB%E5%8D%A6/"},{"name":"总结","slug":"总结","link":"/categories/%E6%80%BB%E7%BB%93/"}]}